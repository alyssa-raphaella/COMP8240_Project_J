\begin{frame}
\frametitle{Project Overview Revisited}
\textbf{Paper}:
\begin{itemize}
    \item Multimodal Speech Emotion Recognition Using Audio and Text by Seunghyun Yoon, Seokhyun Byun, Kyomin Jung
    \item Github link: github.com/david-yoon/multimodal-speech-emotion
    \item 2018 IEEE Spoken Language Technology Workshop (SLT)
    \item Rank 14 in Computational Linguistics Category on Google Scholar
\end{itemize}
\textbf{Four models}: Text only, Audio only, Multimodal, Multimodal-Attention 

\textbf{Processed input data}:
    \begin{itemize}
        \item Speech Data (.npy file)
            \begin{itemize}
                \item Mel Frequency Crystal Coefficients (MFCC)
                \item Prosodic Features
            \end{itemize}
        \item Text Data (.npy file): Word Tokens
        \item Emotion Categories: Angry, Happy, Sad Neutral
    \end{itemize}
    
\textbf{Set up for Google Colab}:
    \begin{itemize}
        \item tensorflow==1.4; python==2.7
        \item scikit-learn==0.20.0; nltk==3.3
        
    \end{itemize}

\end{frame}